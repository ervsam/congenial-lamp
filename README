EXPERIMENT SETUP
--------------------------------------------------------------------------------
1. window size = 3
2. FOV = 7

AGENT'S FOV
shape: (7, 7, 7) = (channel, FOV, FOV)
input to network consists of:
1. surrounding obstacles
2. agent's heuristic (normalized distance to goal)
3. combined neighbors' heuristics (resembling "hot spots")
4-7. DHC heuristics

MODEL ARCHITECTURE
- encoder
    - Conv2d
    - Conv2d
    - Linear
    - Linear

1. Q indiv: agent's FOV -> pair Q indiv
    - encoder
    - MultiheadAttention
    - Linear

2. Q joint: chosen Q indiv -> Q joint val
    - Linear
    - Sum
    - Linear
    - Linear

3. V joint
    - Sum
    - Linear
    - Linear

LOG
--------------------------------------------------------------------------------
## 17 FEB 2025
1. training on warehouse map 25 agents
- test results:
- 35 agents
- model throughput: 4.385
- random throughput: 4.32

2. use PBS for hard instances
- fixing implementation
- assert new_start is not None
- after reaching one goal, push to heap (open_set, (heuristic(current, goal), g, current, path)) not (open_set, (heuristic(current, goal), 0, current, path))
- order of actions in st_astar has to match PBS implementation since tie break is going to affect PP
- training on random_w_PBS with 19 agents

3. use Qjt
. see if Qjt has expressive enough by just training Qjt first

4. TODO
- localize reward to 1 step away from pair
- use curriculum learning (number of agents)

## 18 FEB 2025
1. use PBS for hard instances
- fixing implementation
- should only replan for aj where there exist ak < aj in partial prio N

## 19 FEB 2025
1. use PBS for hard instances
- fixed, plan conflict free only for window_size time step

## 22 FEB 2025
1. train again, with corrected PBS and DHC heur layer, use curriculum learning
. on warehouse_1 map (4 rows, 2 col), train-warehouse_1

## 23 FEB 2025
1. generating result for PBS warehouse_2 in gen_results_warehouse2

2. just realised that fulfillment warehouse map should go back and forth, but checked RHCR code and thats not the case. they just keep generating random goal locations from the shelves, not the start cells
- so why is their throughput so low?
- forgot to divide by window_size = 3

## 3 MAR 2025
1. generating results for random maps

2. train on random map using curriculum learning

## 4 MAR 2025
1. generating result for random_PP 70 agents on warehouse_2
. changed np.random.shuffle to random.shuffle

2. 5000 steps means 5000 agent steps, not 5000 * window steps

# meet updates
1. use Qjt
2. use exp(1.1, delay)*10 as reward
- fix range to [0, 10]
- makes Qjt easier to learn and converge to <1
3. Q'jt is very small and V(s) close to reward because there isnt much difference between Q'jt(s, ui) and Q'jt(s, uj), meaning taking joint action ui vs uj dont make that much of a difference

## 6 MAR 2025
1. why is leakyrelu set to inplace=True in encoder? changed to default


## 28 MAR 2025
1. training loss = Ltd to see if Qjt network is sufficient to learn reward
- based on previous experiment, it does work. but after adding Lopt Lnopt it doesnt? wait for more steps and conclude

## 10 APR 2025
Model input update: Added the agent's coordinates as part of the input.
batch size: 32
Exploration strategy: Switched to epsilon-greedy.
Curriculum learning:
    Started training with 50 agents.
    Incrementally added 2 agents each time the loss converged below 3.
    Training stopped at 128 agents (reason unconfirmed, possibly due to increased difficulty).
Training performance:
    Loss converged well (near 0), as shown in the plot.
QTRAN modification:
    Decreased the lambda parameter (weight of Lopt and Lnopt loss).
    Possibly improved effectiveness of Ltd and Qjoint learning (needs further verification).
Input size adjustment: Changed to 11x11 to allow a simulation window of 5 time steps.
Evaluation:
    Compared trained model to random PP; results shown in the second plot.
    Both methods failed on 100-agent testing scenarios.
    RHCR paper achieved up to 140 agents due to longer planning horizon (20 steps vs. your 5 for planning and 5 for execution).


## 11 APR 2025
1. training Qjt only
- reward is sum of delays, see if Qjt is able to converge
- if so, sum of delays is a bigger value, so difference between rewards given a different action should be larger and this will propagate to the individual q values
- also sum of delays is not affected by number of agents, so hopefully delay distribution is more accurate
. if Qjt converges, then add Lopt and Lnopt and learn to decompose the reward, with and without PBS warm start

2. is the building of DirectedGraph not sorted based on confidence this whole time?
- for u, v in list(pair_qval.keys()):
- pair_qval already sorted first

## 13 APR 2025
1. training Qjt only
- didnt converge
. look into it

## 15 APR 2025
1. training Qjt only
- look at input, try adding global state?
- training using huber loss

a. instead of ave reward * 10, times 100 instead? since in a group theres lots of agents
b. train from very small number of agents first

2. PBS training
- using pbs labels fully not good idea?
- randomize the toposort
- change reward to times 100 instead of 10
- too big
- num_agents/len(group)
- back to times 10
- is model able to learn with such a variety of feasible priority order?
- increase batch size? since loss is so noisy

3. train whole using huber loss?
- training

4. is prioritized replay correctly implemented?

5. what happened to pbs at agents = 90

## 23 APR 2025
1. work on penalize when ordering fails
- given -10 penalty to group that fails

2. is buffer implemented correctly? try save to txt every 100 steps the last 500 data points
- looks like it is

3. PBS trained
. train again with penalized order
- start epsilon from 0.8

4. huber loss doesnt seem better

5. what happened to pbs at agents = 90
- using planning window of 20 like in the paper the throughput keeps increasing
- this suggests that using planning window of 5 the paths planned are not optimal because they are shortsighted

6. train non PBS now with penalized failed order

## 24 APR 2025
1. debug, start from small scenarios, confirm that model is working

                lculated for QTRAN-alt? theres a Qjt term for each agent?

## 29 APR 2025
1. confirmed that model is able to converge on overfit test
. moving to see if Ltd converge on warehouse_0, small warehouse 4 agents

2. LR 1e-3 seems to be doing good for Ltd
. trying hidden dim = 128 since new data cant be predicted well whereas the data in buffer is predicted accurately, tells me that model isnt able to generalize to new data
- increasing hidden dim doesnt seem to help, it gets some delayed predictions correct, i think the problem is that the input is not good enough.

# 3 MAY 2025
1. since i am dividing the loss by the number of groups in an episode, eps with more groups will contribute less loss each group. so if one group in the episode is not doing well, the error is spread among the other groups in that episode. solution, each group has to have equal weight, treated independently.

# 5 MAY 2025
1. is the problem overfitting? or input not good enough?
- try using a simpler model. should help with overfitting

# 6 MAY 2025
1. running small training to see if dev loss per bucket decreases
- fixing the ltd calculation, each group now contributes equally to the loss instead of per episode, so groups in episodes with more groups doesnt contribute less to the loss

2. input global view to Qjt

# 8 MAY 2025
1. train PBS for a very long time, like the first version, until Ltd converge

2. training on small map with 10*ave_reward to see if it converges. uses updated group wise loss
. training on tmux train_small_debug into warehouse_0_a

3. train non PBS for a very long time on warehouse_2, with updated loss calculation on the group level. uses new simpler model. use curriculum learning that starts at 50 agents, and epsilon greedy
. training on tmux train into warehouse_2_update1
. training on tmux ___ into warehouse_2_update1gpu

# 9 MAY 2025
so right now im waiting on the training of the non PBS version with epsilon greedy, trying to see if i get the same better performance in throughput as last time. so i just hv to wait for the results. but from the small map experiment it looks like the Qjt network is not expressive enough? so i think i'll have to use a more complex model. and also in the meantime i can work on including the whole map as the input to the Qjt network so that its able to predict the reward more accurately.

1. train non PBS for a very long time on warehouse_2, with updated loss calculation on the group level. uses new simpler model. use curriculum learning that starts at 50 agents, and epsilon greedy
. training on tmux train into warehouse_2_update1
. training on tmux train_gpu into warehouse_2_update1gpu

2. training on small map with 10*ave_reward to see if it converges. uses updated group wise loss.
- Ltd only first to see if model expressive enough
- epsilon greedy off
- curriculum off
- gpu on
. training on tmux train_small_debug into warehouse_0_a

# 10 MAY 2025
the problem right now is the performance of the model is not much better than just random PP. we can learn the PBS labels so that the model performs close to PBS.
the issue right now is that the model cant even learn the Qjt value accurately. i think this happens after adding the penalty.

1. check if Qjt is better without penalty, just discard infeasible ordering
. training on tmux train_nopenalty into warehouse_2_nopenalty

2. checking if implementation is correct

# 12 MAY 2025
1. tmux train_small_debug into warehouse_0_a
- 175000 steps, Ltd converge, prediction in checkpoint still gets some values wrong.

2. tmux train_gpu into warehouse_2_update1gpu
- 16401 steps, checkpoint doesnt accurately predict Qjt, possibly because model too small, and using -10 confuses the model

3. tmux train into warehouse_2_update1
- 54401 steps, checkpoint doesnt accurately predict Qjt, possibly because model too small, and using -10 confuses the model

4. tmux train_nopenalty into warehouse_2_nopenalty
- 20701 steps, checkpoint not accurate, model possibly too small since penalty is already removed

the Qjt doesnt converge like before. could be because i
1. changed the model or
2. the group wise loss calculation
GOAL: get the model to converge again
TODO now:
- work on the model, get it close to before, at the same time make it efficient (remove python for loops, etc.)

1. train again for long time
- warehouse_2
- epsilon_greedy
- tmux train to warehouse_2_nopenalty

2. is the model still too weak? train with over complex model to see if thats the problem
- tmux train_bigmodel to warehouse_2_bigmodel

# 13 MAY 2025
1. train again for long time
- warehouse_2
- epsilon_greedy
- tmux train to warehouse_2_nopenalty
- Loss: 40.242 LTD: 1.826 LOPT: 34.87 LNOPT-min: 41.962
- 16K steps, loss doesnt converge
- Qjt not predicting accurately

2. is the model still too weak? train with over complex model to see if thats the problem
- tmux train_bigmodel to warehouse_2_bigmodel
- 8K steps, loss converge to ~10 but Qjt always predicts around -2

! is the Ltd calculation wrong?
- test with overfit

3. debug, overfit to see if Ltd is calculated correctly and Qjt can learn, by freezing PER
- tmux debug to warehouse_2_debug
= Qjt can learn and converges after 900 steps, takes longer because the loss calculation is group-level. revert back to pair level treating each pair independently so that each pair learns to predict Qjt faster.
![loss-plot](image.png)

- run again with updated per-pair Ltd calculation
= seems better, the predicted values in a group are similar meaning they are learning together
= the -0.2 is inaccurate tho
![loss-plot](image-1.png)

Ltd loss is correct, but it takes 1K steps for Ltd just to be less than 0.1
- increase learning rate, use scheduler
- group calculation wrong?
[[-3.847, -3.794, -3.855, ..., -3.802, -3.765], [-3.674]] --------------------- [-3.742, -0.2]
[[-1.149, -1.116, -1.385, -1.1, -1.023, -1.046], [-1.271, -1.058, ..., -1.169, -0.994, -1.356], [-1.215, -1.087, -1.014, -1.052, ..., -0.853, -1.051]] --------------------- [-0.2, -1.45, -0.533]
[[-1.955, -1.922, -1.99, -2.019, -2.083, -2.03, ..., -1.844, -1.906, -2.0, -2.15, -1.96], [-1.978, -1.971, -1.97, ..., -1.763, -1.642, -2.048, -2.075], [-1.926]] --------------------- [-2.568, -0.2, -0.2]

- increased learning rate from 1e-4 to 1e-3
= 400 steps for Ltd to be less than 0.1
![loss-plot](image-2.png)

4. group calculation wrong?, change PER to store groups not episodes
- overfit again to see if the -0.2 values are predicted correctly.
- tmux debug to warehouse_2_debug
= 325 steps, Ltd less that 0.1
![loss-plot](image-3.png)

5. train Qjt only (non-overfit) warehouse_2
- no penalty
- tmux train_Ltd to warehouse_2_LTDonly
= 8K steps, Ltd converge to less than 0.1
![loss-plot](image-4.png)

6. train full loss warehouse_2, epsilon greedy
- tmux train_full to warehouse_2_nopenalty
= 8K steps, Ltd converge ~2
= Lopt Lnopt doesnt converge, i think the loss calculation has a problem?
![loss-plot](image-6.png)

7. train full loss warehouse_2, where Lopt and Lnopt-min is per pair
- tmux train_full_pair to warehouse_2_nopenalty_pairloss
= 8K steps, Lopt doesnt converge
![loss-plot](image-5.png)


Lopt, Lnopt-min not converge
-0.2 prediction not accurate

# 14 MAY 2025
>1. train again to see if the losses converge
- changes: .mean() over batch, instead of weighted mean over IS weights
            '''
            per_pair_w = torch.repeat_interleave(weights_tensor, torch.tensor(num_n_pairs, device=self.device))
            per_pair_w = per_pair_w / (per_pair_w.max() + 1e-8)
            ltd = (per_pair_w * ltd_raw).mean()
            '''

> debugging to see if shapes for loss calculations are correct, tmux debug to warehouse_0_debug
= wrong shape for lopt, needed to squeeze before multiply with weights

> tmux train_full to warehouse_2_nopenalty

>2. -0.2 delays are not predicted correctly. does this have to do with PER not sampling enough since the group size is small? but new weight is calculated by max of td error
i can check by running overfit test, each pair should update together
- doesnt get trained as much? because of the td_error? fix: try deactivate PER by setting the hyperparameter alpha(?) to 1 or something
- bigger groups have more weight in loss? i dont think so
= turns out its a shape problem, .squeeze() selected_Q
pair_td = (selected_Q.squeeze() - batch_local_rewards).abs().detach()    # (total_pairs,)

\\ QTRAN-alt Lnopt-min should min(., 0)? In summary, the implementation should ensure that only negative violations are penalized (e.g. by something like torch.clamp(min_diff, max=0) before squaring).

> line 363 not updated yet to predict per group?
q_jt, q_jt_alt = trainer.qjoint_net(pair_enc, batch_pair_enc_action.to(device), [close_pairs], [groups], [len(partial_prio)])
it should be fine right?
__CHANGE__: try train again with fixed td_error calculation for PER
> tmux train_full_fixed to warehouse_2_debug

3. train using PBS labels
> tmux train_pbs to warehouse_2_PBS

# 15 MAY 2025
1. tmux train_full to warehouse_2_nopenalty
- Step 17400, training at 84 agents, loss looks good
Loss: 0.249 LTD: 0.126 LOPT: 0.085 LNOPT-min: 0.162
![loss-plot](image-7.png)

> prediction of multiple groups not correct
    Q-joint chosen:
    [-6.128, -6.087, ..., 0.502, -6.072, -6.108]
    [-6.136, -6.075, -6.041]
    Local Rewards: [-6.867, -0.2] 

    Q-joint chosen:
    [-5.429, -5.486, ..., -5.483, -5.36, -5.614]
    [-5.912, -5.455, -5.449]
    Local Rewards: [-6.253, -3.533] 

    Q-joint chosen:
    [0.534]
    [-3.658, -3.692, -3.672, ..., -3.682, -3.667]
    Local Rewards: [-0.2, -4.944] 

2. tmux train_full_fixed to warehouse_2_debug
![loss-plot](image-8.png)
- prediction of multiple groups also not correct
- Ltd less than 1, predictions of Qjt pretty accurate

> Step 13833, Q-vals predicted not useful?
Q-vals:
(0, 4) [-0.01 -0.01]
(0, 5) [-0.01  -0.009]
(0, 6) [-0.01  -0.009]
(0, 16) [-0.011 -0.009]
(0, 20) [-0.01  -0.009]
(0, 24) [-0.01  -0.009]
(0, 31) [-0.01  -0.009]
(0, 33) [-0.01  -0.009]
(0, 38) [-0.01  -0.009]
(0, 45) [-0.01  -0.009]

3. tmux train_pbs to warehouse_2_PBS
- also has multiple group prediction problem
- i dont think its just a bad prediction of -0.2 because the pred values are close to the other group, for ex:
Q-joint chosen:
[-2.331, -2.249, ..., -2.327, -4.628]
[-2.273, ..., -2.364]
Local Rewards: [-1.386, -0.2] 

Q-joint chosen:
[-1.612, -1.576, ..., -3.329]
[-1.537]
Local Rewards: [-1.006, -0.2] 
- so i think there is a bug in the inference

= 10K steps, Ltd and Qjt preds look fine
Loss: 0.156 LTD: 0.129 LOPT: 0.026 LNOPT-min: 0.028

= also no clear useful values from Q-vals
Q-vals:
(0, 14) [0.004 0.005]
(0, 16) [0.004 0.004]
(0, 17) [0.004 0.004]
(0, 20) [0.004 0.004]
(0, 21) [0.003 0.003]
(0, 24) [0.003 0.003]
(0, 26) [0.003 0.003]
(0, 40) [0.004 0.004]
(0, 53) [0.004 0.004]
(0, 54) [0.004 0.004]
(0, 55) [0.004 0.003]
(0, 57) [0.004 0.004]
(0, 61) [0.003 0.004]
(1, 2) [0.002 0.002]

__CHANGE__: set Trainer self.update_every = 10 to 200

> check inference in train.py
q_jt, q_jt_alt = trainer.qjoint_net(pair_enc, batch_pair_enc_action.to(device), [close_pairs], [groups], [len(partial_prio)])
__CHANGE__: QJoint.forward() is wrong in regrouping the alt_q
    # OLD (DELETE) - wrong split
    # # reorganise alt_q into (G_i, n_pairs_in_g, 2) for convenience
    # splits   = [len(g) for g in groups]
    # alt_per_g = alt_q.split(splits, dim=0)

    # reorganise alt_q into (G_i, n_pairs_in_g, 2) by explicit group membership
    # map each pair tuple to its row index
    alt_per_g = []
    for group in groups:
        idxs = [pair2idx[p] for p in group]
        alt_per_g.append(alt_q[idxs])


4. tmux debug to warehouse_2_noPBS
- fix inference
__CHANGE__: k1_div_len_g calculated incorrectly, used scatter_mean(torch.ones_like(key1), …) averages a tensor of all 1’s per feature, so it always gives you back 1
counts = scatter_sum(torch.ones(len(pairs), device=key1.device),
                     group_index, dim=0)          # → (G_i,)
# expand counts back to per-pair
count_per_pair = counts[group_index].unsqueeze(-1) # → (P_i,1)
k1_div_len_g = key1 / count_per_pair               # → (P_i, D)
> tmux debug to warehouse_2_noPBS
> then check if the multigroups preds are correct

> tmux train_pbs to warehouse_2_PBS, with the above fixes

> tmux results_qtrannopbs to qtran 60-90 using q_net_15102.pth
> tmux results_qtranpbs to qtran_pbs 60-90 using q_net_15096.pth
not better than random PP

# 16 MAY 2025

> tmux train_sumrewardnormalized to warehouse_2_sumdelaynormalized

- learning rate? print
- remove -0.2 in reward

- add penalty again
- try use negative sum of delay as the reward
- try train for N (ex. 80) agents only and test instead of using curr learning

- test on a very small map

- why does QTRAN-alt predict Qjt per agent?
> because architecture uses counterfactual network

# 17 MAY 2025

> my_module.py _get_fov was not updated yet to pad constant value of 0. check for other mistakes

# 20 MAY 2025

- tmux train_sumrewardnormalized to warehouse_2_sumdelaynormalized
- tmux train_sumreward to warehouse_2_sumdelay
- tmux train_pbs to warehouse_2_PBS
- tmux debug to warehouse_2_noPBS
= training with PBS labels gives better performance, but still far from PBS throughput

1. build dataset, either random PP or PBS, and train using that
- will be faster, since generating data is fast, and training will be faster since it doesnt wait for env.step()

2. try overfit test with PBS labels? show that training using PBS labels will lead to good throughput?

# 26 MAY 2025
- tmux train_sumrewardnormalized to warehouse_2_sumdelaynormalized
- Qjt preds for new data still not accurate
![loss plot](image-9.png)

- tmux train_sumreward to warehouse_2_sumdelay
- Qjt preds not accurate even in buffer
![loss plot](image-10.png)

- tmux train_pbs to warehouse_2_PBS
- same issue, Qjt not predicting correctly

- tmux debug to warehouse_2_noPBS
- same issue

1. train supervised learning PBS, using the same inputs

2. use attention mechanism for Qjt
- tmux train_ltdonly_att to warehouse_2_LTDonly_attention

3. train supervised learning PBS using manual features to see if there is an improvement compared to random PP

4. train Qjt only, see if it can accurately predict new data. if it does, then freeze model and use to train Lopt
- tmux train_ltdonly to warehouse_2_LTDonly
- should converge after 20K eps

## meeting
1. manual credit assignment, look at lower priority agent how much it is delayed, assign the delay to the pair involved
2. supervised learning using PBS
3. input: provide info about the nearby agents, hoping Qjt value more accurate
4. global info to Qjt
5. increase model size

# 29 MAY 2025
1. tmux train_ltdonly_att to warehouse_2_LTDonly_attention
1. tmux train_ltdonly to warehouse_2_LTDonly
- both converge, but unseen data not predicted correctly

2. supervised learning PBS
- waiting throughput results
- trained on 1000 samples, 20 epochs, 70 agents
- not able to solve 5000 steps in 70 agents
- train again with more samples

3. increase model size
- tmux train_ltdonly to warehouse_2_LTDonly_attention
- training, see if loss decreases
> is delay calculation correct?

# 30 MAY 2025
1. supervised learning PBS
- train on more samples

2. increase model size
- model calculation? there was a mistake, include goals that are within window_size but always calculated dist from start point, not cummulative from start to g1 to g2
- training, see if loss decreases
- tmux train_ltdonly to warehouse_2_LTDonly_attention

3. global info to Qjt

4. input: provide info about the nearby agents, hoping Qjt value more accurate
- tmux train_neighbor to warehouse_2_neighborview

## meeting
1. compare predicted order from supervised to actual PBS order

2. why the predicted values is not between [0, 1]

3. manually decompose and use as reward for independent q learning

4. train from total order? or just from PBS label and set tie to another class

## 5 JUN 2025
1. tmux train_neighbor to warehouse_2_neighborview
- having problems with training
- if len(dev_buffer) >= BATCH_SIZE:
- solved
- tmux train_neighbor to warehouse_2_neighborview

2. supervised learning PBS
- fixed mistake: trained on whole dataset including test set
- train on more samples
- used early model with 60% test acc on RHCR and got 2.176 throughput for 70 agents, QTRAN_PBS 2.175 and WHCA 2.132
- generating 100 000 samples

## 6 JUN 2025
1. supervised learning PBS
__CURRENT__: train on more samples
- generating 100 000 samples
TODO:
- check accuracy
- check throughput
- check if predicted order matches PBS order
- check prediction is between 0 and 1

2. tmux train_neighbor to warehouse_2_neighborview
- there was an error in write_buffer, solved by removing, training again
__CURRENT__: tmux train_neighbor to warehouse_2_neighborview
TODO:
- see if Qjt pred of new samples is accurate

3. train from total order? or just from PBS label and set tie to another class
__CURRENT__: tmux train_supervise_partialprio
- most labels are 2 (tie), so accuracy is really high
- Test label 0: 80 1: 45 2: 6354


## 13 JUN 2025
1. predict partial order and run pbs

## 17 JUN 2025
GOAL:
1. decrease the number of nodes expanded
    - accurate partial priorities assigned from the start


sanity check: if i assign the correct pbs prio from the start, num of nodes expanded should be 0
checked

1. train 3 class model, with balanced class


## meet
1. check runtime of model prediction
