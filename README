EXPERIMENT SETUP
--------------------------------------------------------------------------------
1. window size = 3
2. FOV = 7

AGENT'S FOV
shape: (7, 7, 7) = (channel, FOV, FOV)
input to network consists of:
1. surrounding obstacles
2. agent's heuristic (normalized distance to goal)
3. combined neighbors' heuristics (resembling "hot spots")
4-7. DHC heuristics

MODEL ARCHITECTURE
- encoder
    - Conv2d
    - Conv2d
    - Linear
    - Linear

1. Q indiv: agent's FOV -> pair Q indiv
    - encoder
    - MultiheadAttention
    - Linear

2. Q joint: chosen Q indiv -> Q joint val
    - Linear
    - Sum
    - Linear
    - Linear

3. V joint
    - Sum
    - Linear
    - Linear

LOG
--------------------------------------------------------------------------------
## 17 FEB 2025
1. training on warehouse map 25 agents
- test results:
- 35 agents
- model throughput: 4.385
- random throughput: 4.32

2. use PBS for hard instances
- fixing implementation
- assert new_start is not None
- after reaching one goal, push to heap (open_set, (heuristic(current, goal), g, current, path)) not (open_set, (heuristic(current, goal), 0, current, path))
- order of actions in st_astar has to match PBS implementation since tie break is going to affect PP
- training on random_w_PBS with 19 agents

3. use Qjt
. see if Qjt has expressive enough by just training Qjt first

4. TODO
- localize reward to 1 step away from pair
- use curriculum learning (number of agents)

## 18 FEB 2025
1. use PBS for hard instances
- fixing implementation
- should only replan for aj where there exist ak < aj in partial prio N

## 19 FEB 2025
1. use PBS for hard instances
- fixed, plan conflict free only for window_size time step

## 22 FEB 2025
1. train again, with corrected PBS and DHC heur layer, use curriculum learning
. on warehouse_1 map (4 rows, 2 col), train-warehouse_1

## 23 FEB 2025
1. generating result for PBS warehouse_2 in gen_results_warehouse2

2. just realised that fulfillment warehouse map should go back and forth, but checked RHCR code and thats not the case. they just keep generating random goal locations from the shelves, not the start cells
- so why is their throughput so low?
- forgot to divide by window_size = 3

## 3 MAR 2025
1. generating results for random maps

2. train on random map using curriculum learning

## 4 MAR 2025
1. generating result for random_PP 70 agents on warehouse_2
. changed np.random.shuffle to random.shuffle

2. 5000 steps means 5000 agent steps, not 5000 * window steps

# meet updates
1. use Qjt
2. use exp(1.1, delay)*10 as reward
- fix range to [0, 10]
- makes Qjt easier to learn and converge to <1
3. Q'jt is very small and V(s) close to reward because there isnt much difference between Q'jt(s, ui) and Q'jt(s, uj), meaning taking joint action ui vs uj dont make that much of a difference
