EXPERIMENT SETUP
--------------------------------------------------------------------------------
1. window size = 3
2. obstacle density = 0.5
3. FOV = 7
4. map size = FOV*2 x FOV*2

AGENT'S FOV
shape: (7, 7, 7) = (channel, FOV, FOV)
input to network consists of:
1. surrounding obstacles
2. agent's heuristic (normalized distance to goal)
3. combined neighbors' heuristics (resembling "hot spots")
4-7. DHC heuristics

MODEL ARCHITECTURE
- encoder
    - Conv2d
    - Conv2d
    - Linear
    - Linear

1. Q indiv: agent's FOV -> pair Q indiv
    - encoder
    - MultiheadAttention
    - Linear

2. Q joint: chosen Q indiv -> Q joint val
    - Linear
    - Sum
    - Linear
    - Linear

3. V joint
    - Sum
    - Linear
    - Linear


THINGS TO DO
--------------------------------------------------------------------------------
TODO: compare with random PP as baseline and better PP by running 10 times and choosing the best one

LOG
--------------------------------------------------------------------------------

2 JAN 2025

- finished training, didnt give correct individual Q values
- design overfit test 1: 2 agents, A->B is optimal

training:
- train using Vjoint too because its needed when partially observable
- lnopt_min uses fixed_q_jt_alt instead of q_jt_alt

training (1):
- update encoder by adding layers, may be too simple
- linear layer linear_0 for joint_qs_prereduced updated by adding layers
- mistake: when calculating b_qjt_alt, key1 should divide n_agents, not n_actions

results:
- base: takes 8K steps for Ltd to converge to less than 1. then indiv Q starts to be correct


- DEC 28: training QTRAN-base again without vtot on updated model layers
- Ltd converge to less than 1 after 3600 steps, without vtot. so whats important is expressive model (more linear layers)
- training on real simulation
- error on assigning goals
Traceback (most recent call last):
  File "/local-scratch/localhome/esa95/Desktop/experiments/DEC 28/train.py", line 478, in <module>
    global_reward = sum([-x for x in env.get_delays()])
  File "/local-scratch/localhome/esa95/Desktop/experiments/DEC 28/Environment.py", line 409, in get_delays
    ][goals[n_goals_reached+i+1][0], goals[n_goals_reached+i+1][1]]
IndexError: list index out of range

- JAN 02: training again using QTRAN-alt up to 8K steps
- losses not converging, start looking from Ltd.
- there is a bug in lnopt_min, q_is should take from batch_q_vals not selected_Q
- training again

- QTRAN-alt WORKS ON OVERFIT TEST
- Ltd less than 1 at 3040 steps

future:
- use idx of agent as input to model to differentiate agents

- fixed env.get_delays() by using new calculation: len(actual_path) - len(opt_path)

- train MAPF simulation using QTRAN-alt
- RESULT:
    - stopped because 
    Traceback (most recent call last):
    File "/local-scratch/localhome/esa95/Desktop/experiments/JAN 02/train.py", line 462, in <module>
        batch_pair_enc_action = torch.concat([pair_enc, batch_onehot_actions], dim=1)
    RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 4 for tensor number 1 in the list.

    length of pair_enc is 28 and length of batch_onhot_actions is 4? how?
    answer: when there is an unresolvable cycle, env resets but keeps taking step, should return None instead and move on to next time step

    - lots of "delays and updated_delays are not the same"
        - space time astar wait_action argument making optimal path calculation wrong because optimal path doesnt use wait action (staying still). this was implemented to make stA* faster previously, but turns out it was slow because it was implemented incorrectly. so its not unecessary
        - still lots of delays and updated_delays that are different, based on printed actual paths and optimal paths we should use len(actual) - len(optimal) instead


4 JAN 2025

- train MAPF simulation using QTRAN-alt (trained in JAN 2 folder)
- RESULT:
    - Ltd stays around 7 even after 201K steps

- train MAPF simulation using QTRAN-base (trained in JAN 4 folder)
- RESULT:
    - Ltd stays around 15 even after 180K steps


5 JAN 2025

- i think the best thing to try is to break the groups into localized shared reward, this will make instances smaller so model hopefully doesnt have to learn many things
    - a group of agents is agents that are connected
    - rewards is sum of delay in group
    - group as input to Qjoint net

- fixed: env._get_neighboring_agents() close pairs should be if dist_heur(A, B) <= window_size*2 not fov

- qjoint net needs to be more expressive?
- try on simpler instance? or curriculum learning

7 JAN 2025

- verify implementation of QTRAN-base localized reward
- overfit test with two groups

- work on QTRAN-alt localized reward
- overfit test with two groups

- train MAPF simulation using QTRAN-alt localized reward


13 JAN 2025

- normalize reward? try on overfit test first

14 JAN 2025

- train on current map but with more agents, see if it can solve instances random PP fails to solve
    - find number of agents random PP starts to fail

- penalize cycles
- train on warehouse map with 300 agents
- improve network architecture


19 JAN 2025
- training again with more linear layers on qjoint net
- PBS impl still wrong? not returning lowest cost, might not be replanning for lower prio agents
    - check low_level_search

20 JAN 2025
- Qjt is learned to estimate return (reward + future value), but we dont have future value. so can we replace Qjt and directly use global reward for Lopt and Lnopt?
    - for Lopt, need to replace Qjt(u_bar) with optimal local rewards
    - try on overfit test

26 JAN 2025
- found out there was a bug, BUFFER_SIZE set to 32 not 10000. fixed.
- trained simulation of 6 agents

Step 387830 

Q_vals:
(0, 3) [-0.032 -0.031]
(1, 2) [-0.291 -0.331]
(1, 4) [-0.246 -0.153]
(1, 5) [-0.094 -0.067]
(2, 4) [-1.452 -1.014]
(2, 5) [-1.368 -1.095]
(4, 5) [0.339 0.308]

Environment.step(): Agent 1 reached goal 0
Time to env.step: 0.048
Priority ordering: [3, 4, 0, 5, 2, 1] 

Time to solve instance: 0.057 

Q'jt: -2.352
Q-joint predicted:
[0.213]
[-2.098, -2.39, -2.259, -2.031, -2.027, -2.351]
Q'jt - Qjt:
[-2.564]
[-0.254, 0.038, -0.093, -0.321, -0.325, -0.001]

Global Reward: -1 

Multiple groups detected
Groups: [[0, 3], [1, 2, 4, 5]]
Delays: [0, 0, 1, 0, 0, 0]
Local Rewards: [0, -1] 

Loss: 2.293 LTD: 0.972 LOPT: 0.418 LNOPT-min: 0.903

- found mistake in test.ipynb
    - env.starts = start_locations[idx].copy(), env.goals = goal_locations[idx].copy()
    - need to use .copy(), it's been changing the start_locations and goal_locations


27 JAN 2025
- tested 6 agents trained model on test set, compared with random PP, 1000 instances, throughput is similar at 0.528 and 0.523 respectively

- results of training on 6 agents saved in log_6agents.txt, loss_plot_6agents.png, losses_6agents.png

- results of overfit using PrioritizedReplay after 8K steps saved in overfit_prioritizedreplay.txt, loss_plot_overfit_prioritizedreplay.png

28 JAN 2025
- waiting for train.py on 15 agents
    - Loss: 55.037 LTD: 46.662 LOPT: 4.313 LNOPT-min: 4.062
    - at 12980 steps

- use real global reward directly

- waiting for train using complex model

- unfix Qjt in Lopt and Lnopt like in QTRAN++
- use QTRAN++ loss calculations


30 JAN 2025
- tested trained model of 15 agents on test set and compared to random PP
- throughput:
    - model : 1.166
    - random : 1.139
- Loss: 10.844 LTD: 9.206 LOPT: 0.819 LNOPT-min: 0.819
- at 168400 steps

- changed
- self.params = list(self.q_net.parameters()) + list(self.qjoint_net.parameters()) + list(self.vnet.parameters())
- to
- self.params = list({p for net in [self.q_net, self.vnet] for p in net.parameters()})
- because first one might contain duplicates since we are sharing parameters

- retraining overfit with updates above to see if Lopt and Lnopt can converge to 0