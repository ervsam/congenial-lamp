EXPERIMENT SETUP
--------------------------------------------------------------------------------
1. window size = 3
2. obstacle density = 0.5
3. FOV = 7
4. map size = FOV*2 x FOV*2

AGENT'S FOV
shape: (7, 7, 7) = (channel, FOV, FOV)
input to network consists of:
1. surrounding obstacles
2. agent's heuristic (normalized distance to goal)
3. combined neighbors' heuristics (resembling "hot spots")
4-7. DHC heuristics

MODEL ARCHITECTURE
- encoder
    - Conv2d
    - Conv2d
    - Linear
    - Linear

1. Q indiv: agent's FOV -> pair Q indiv
    - encoder
    - MultiheadAttention
    - Linear

2. Q joint: chosen Q indiv -> Q joint val
    - Linear
    - Sum
    - Linear
    - Linear

3. V joint
    - Sum
    - Linear
    - Linear


THINGS TO DO
--------------------------------------------------------------------------------
TODO: compare with random PP as baseline and better PP by running 10 times and choosing the best one

LOG
--------------------------------------------------------------------------------

2 JAN 2025

- finished training, didnt give correct individual Q values
- design overfit test 1: 2 agents, A->B is optimal

training:
- train using Vjoint too because its needed when partially observable
- lnopt_min uses fixed_q_jt_alt instead of q_jt_alt

training (1):
- update encoder by adding layers, may be too simple
- linear layer linear_0 for joint_qs_prereduced updated by adding layers
- mistake: when calculating b_qjt_alt, key1 should divide n_agents, not n_actions

results:
- base: takes 8K steps for Ltd to converge to less than 1. then indiv Q starts to be correct


- DEC 28: training QTRAN-base again without vtot on updated model layers
- Ltd converge to less than 1 after 3600 steps, without vtot. so whats important is expressive model (more linear layers)
- training on real simulation
- error on assigning goals
Traceback (most recent call last):
  File "/local-scratch/localhome/esa95/Desktop/experiments/DEC 28/train.py", line 478, in <module>
    global_reward = sum([-x for x in env.get_delays()])
  File "/local-scratch/localhome/esa95/Desktop/experiments/DEC 28/Environment.py", line 409, in get_delays
    ][goals[n_goals_reached+i+1][0], goals[n_goals_reached+i+1][1]]
IndexError: list index out of range

- JAN 02: training again using QTRAN-alt up to 8K steps
- losses not converging, start looking from Ltd.
- there is a bug in lnopt_min, q_is should take from batch_q_vals not selected_Q
- training again

- QTRAN-alt WORKS ON OVERFIT TEST
- Ltd less than 1 at 3040 steps

future:
- use idx of agent as input to model to differentiate agents

- fixed env.get_delays() by using new calculation: len(actual_path) - len(opt_path)

- train MAPF simulation using QTRAN-alt
- RESULT:
    - stopped because 
    Traceback (most recent call last):
    File "/local-scratch/localhome/esa95/Desktop/experiments/JAN 02/train.py", line 462, in <module>
        batch_pair_enc_action = torch.concat([pair_enc, batch_onehot_actions], dim=1)
    RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 4 for tensor number 1 in the list.

    length of pair_enc is 28 and length of batch_onhot_actions is 4? how?
    answer: when there is an unresolvable cycle, env resets but keeps taking step, should return None instead and move on to next time step

    - lots of "delays and updated_delays are not the same"
        - space time astar wait_action argument making optimal path calculation wrong because optimal path doesnt use wait action (staying still). this was implemented to make stA* faster previously, but turns out it was slow because it was implemented incorrectly. so its not unecessary
        - still lots of delays and updated_delays that are different, based on printed actual paths and optimal paths we should use len(actual) - len(optimal) instead


4 JAN 2025

- train MAPF simulation using QTRAN-alt (trained in JAN 2 folder)
- RESULT:
    - Ltd stays around 7 even after 201K steps

- train MAPF simulation using QTRAN-base (trained in JAN 4 folder)
- RESULT:
    - Ltd stays around 15 even after 180K steps


5 JAN 2025

- i think the best thing to try is to break the groups into localized shared reward, this will make instances smaller so model hopefully doesnt have to learn many things
    - a group of agents is agents that are connected
    - rewards is sum of delay in group
    - group as input to Qjoint net

- fixed: env._get_neighboring_agents() close pairs should be if dist_heur(A, B) <= window_size*2 not fov

- qjoint net needs to be more expressive?
- try on simpler instance? or curriculum learning

7 JAN 2025

- verify implementation of QTRAN-base localized reward
- overfit test with two groups

- work on QTRAN-alt localized reward
- overfit test with two groups

- train MAPF simulation using QTRAN-alt localized reward


13 JAN 2025

- normalize reward? try on overfit test first

14 JAN 2025

- train on current map but with more agents, see if it can solve instances random PP fails to solve
    - find number of agents random PP starts to fail

- penalize cycles
- train on warehouse map with 300 agents
- improve network architecture


19 JAN 2025
- training again with more linear layers on qjoint net
- PBS impl still wrong? not returning lowest cost, might not be replanning for lower prio agents
    - check low_level_search

20 JAN 2025
- Qjt is learned to estimate return (reward + future value), but we dont have future value. so can we replace Qjt and directly use global reward for Lopt and Lnopt?
    - for Lopt, need to replace Qjt(u_bar) with optimal local rewards
    - try on overfit test

26 JAN 2025
- found out there was a bug, BUFFER_SIZE set to 32 not 10000. fixed.
- trained simulation of 6 agents

Step 387830 

Q_vals:
(0, 3) [-0.032 -0.031]
(1, 2) [-0.291 -0.331]
(1, 4) [-0.246 -0.153]
(1, 5) [-0.094 -0.067]
(2, 4) [-1.452 -1.014]
(2, 5) [-1.368 -1.095]
(4, 5) [0.339 0.308]

Environment.step(): Agent 1 reached goal 0
Time to env.step: 0.048
Priority ordering: [3, 4, 0, 5, 2, 1] 

Time to solve instance: 0.057 

Q'jt: -2.352
Q-joint predicted:
[0.213]
[-2.098, -2.39, -2.259, -2.031, -2.027, -2.351]
Q'jt - Qjt:
[-2.564]
[-0.254, 0.038, -0.093, -0.321, -0.325, -0.001]

Global Reward: -1 

Multiple groups detected
Groups: [[0, 3], [1, 2, 4, 5]]
Delays: [0, 0, 1, 0, 0, 0]
Local Rewards: [0, -1] 

Loss: 2.293 LTD: 0.972 LOPT: 0.418 LNOPT-min: 0.903

- found mistake in test.ipynb
    - env.starts = start_locations[idx].copy(), env.goals = goal_locations[idx].copy()
    - need to use .copy(), it's been changing the start_locations and goal_locations


## 27 JAN 2025
- tested 6 agents trained model on test set, compared with random PP, 1000 instances, throughput is similar at 0.528 and 0.523 respectively

- results of training on 6 agents saved in log_6agents.txt, loss_plot_6agents.png, losses_6agents.png

- results of overfit using PrioritizedReplay after 8K steps saved in overfit_prioritizedreplay.txt, loss_plot_overfit_prioritizedreplay.png

## 28 JAN 2025
1. 
- waiting for train.py on 15 agents
    - Loss: 55.037 LTD: 46.662 LOPT: 4.313 LNOPT-min: 4.062
    - at 12980 steps

2. 
- use real global reward directly

3. 
- waiting for train using complex model

4. 
- unfix Qjt in Lopt and Lnopt like in QTRAN++
- use QTRAN++ loss calculations


## 30 JAN 2025
1. 
- tested trained model of 15 agents on test set and compared to random PP
- throughput:
    - model : 1.166
    - random : 1.139
- Loss: 10.844 LTD: 9.206 LOPT: 0.819 LNOPT-min: 0.819
- at 168400 steps

2. 
- changed
- self.params = list(self.q_net.parameters()) + list(self.qjoint_net.parameters()) + list(self.vnet.parameters())
- to
- self.params = list({p for net in [self.q_net, self.vnet] for p in net.parameters()})
- because first one might contain duplicates since we are sharing parameters

3. test overfit without Qjt
- retraining overfit with updates above to see if Lopt and Lnopt can converge to 0
- still not converging to 0 even thought its no longer using Qjt
- step 737, Loss: 7.744 LOPT: 5.558 LNOPT-min: 2.187
- WHY???


## 1 FEB 2025
- found a recent bug, forgot to copy parameters to fixed_qjoint_net
- test overfit again

## 2 FEB 2025
1. test overfit without Qjt
a. why cant Lopt and Lnopt be 0
- confirmed that value of Qjt max is -5, 0
- confirmed that batch_max_local_rewards is also -5, 0
- running overfit.py again
- Q'jt max action is ~ -5, so why is Lopt 5.5?
    - training again while also printing Q'jt, local reward, vtot
    - found mistake, wrong calc of lopt, needed unsqueeze batch_max_local_rewards.unsqueeze(1) otherwise turned into (288, 288) shape instead of (288, 1)
- running overfit.py again
    - Lopt is now < 1
    - but there is mistake in calculating Lnopt-min, cant just use local_reward, that would be QTRAN-base
        - would need to run env.step for both actions
    - implemented env.step for every (u, u_-i), takes very long to run
    - implemented "memory" to keep memory[start][goal] = delay
    - trained again converged to these values at 800 steps
    Q_vals:
    (0, 1) [-0.037  0.13 ]
    (0, 2) [-0.176  0.287]
    (0, 3) [ 0.559 -0.602]
    (1, 2) [-0.268  0.357]
    (1, 3) [ 0.469 -0.545]
    (2, 3) [ 0.575 -0.58 ]
    (4, 5) [ 0.028 -0.139]
    (4, 6) [-0.006 -0.098]
    (5, 6) [ 0.032 -0.115]


2. train.py
- to at least show that with Lopt, Lnopt < 1 (even though Ltd ~10), model has slightly higher throughput than random PP
- train again on 15 agents with the self.params fix, at least 168400 steps
- after 185000 steps Ltd is around 10, sometimes 20
- and throughput compared to random is (1.138, 1.148) respectively
- try not using Qjt, use local reward directly
- at 61584 steps, LOPT: 6.971 LNOPT-min: 5.001, throughput 1.168 model vs 1.144 random

. cleaning up files, then move on to training on warehouse maps

3. 
- train using epsilon greedy
- easier to train on warehouse map
- unfix Qjt in Lopt and Lnopt like in QTRAN++
- normalize delay, or cap max delay to -10
- localize reward to 1 step away from pair
- use QTRAN++ loss calculations

## 8 FEB 2025
clean up
- saved models to trained_models/ folder

1. test on warehouse map
- use the 60K trained model to test

## 13 FEB 2025
1. train.py
. cleaning up files, then move on to training on warehouse maps

2. 
- train using epsilon greedy
- normalize delay, or cap max delay to -10
- localize reward to 1 step away from pair

## 14 FEB 2025
1. use Qjt
- decided to use Qjt because saving it is a terrible idea. too many combinations, if its possible to save them then its possible to just do a look up, no need for training.
. see if Qjt has expressive enough by just training Qjt first

2. use exp(a, delay) as reward
- figured we need to normalize delay to help with training
- small value makes gradient disappear
- 1.1 ** -(sum([delays[agent] for agent in group])) * 10

3. training on warehouse map
- training on 25 agents
. training on warehouse map
. testing 15K model with 27 agents 

4. use PBS for hard instances
- implemented in utils.step
. training on random_w_PBS with 22 agents

5. TODO
- localize reward to 1 step away from pair


## 15 FEB 2025
1. training on warehouse map
. test results:
- 28 agents
- model throughput:
- random throughput:

2. use PBS for hard instances
- fixing problems with implementation
- low_level_search in util should accept multiple goals like st_astar
- then training on random_w_PBS with 20 agents

3. use Qjt
. see if Qjt has expressive enough by just training Qjt first

4. TODO
- localize reward to 1 step away from pair

## 17 FEB 2025
1. training on warehouse map 25 agents
. test results:
- 35 agents
- model throughput: 4.385
- random throughput: 4.32

2. use PBS for hard instances
. fixing implementation
- assert new_start is not None
- after reaching one goal, push to heap (open_set, (heuristic(current, goal), g, current, path)) not (open_set, (heuristic(current, goal), 0, current, path))
- order of actions in st_astar has to match PBS implementation since tie break is going to affect PP
. training on random_w_PBS with 19 agents

3. use Qjt
. see if Qjt has expressive enough by just training Qjt first

4. TODO
- localize reward to 1 step away from pair
- use curriculum learning (number of agents)


## 18 FEB 2025
1. use PBS for hard instances
. fixing implementation
- should only replan for aj where there exist ak < aj in partial prio N


## 19 FEB 2025
1. use PBS for hard instances
- fixed, plan conflict free only for window_size time step



# meet updates
1. use Qjt
2. use exp(1.1, delay)*10 as reward
- fix range to [0, 10]
- makes Qjt easier to learn and converge to <1
3. Q'jt is very small and V(s) close to reward because there isnt much difference between Q'jt(s, ui) and Q'jt(s, uj), meaning taking joint action ui vs uj dont make that much of a difference
