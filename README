EXPERIMENT SETUP
--------------------------------------------------------------------------------
1. window size = 3
2. FOV = 7

AGENT'S FOV
shape: (7, 7, 7) = (channel, FOV, FOV)
input to network consists of:
1. surrounding obstacles
2. agent's heuristic (normalized distance to goal)
3. combined neighbors' heuristics (resembling "hot spots")
4-7. DHC heuristics

MODEL ARCHITECTURE
- encoder
    - Conv2d
    - Conv2d
    - Linear
    - Linear

1. Q indiv: agent's FOV -> pair Q indiv
    - encoder
    - MultiheadAttention
    - Linear

2. Q joint: chosen Q indiv -> Q joint val
    - Linear
    - Sum
    - Linear
    - Linear

3. V joint
    - Sum
    - Linear
    - Linear

LOG
--------------------------------------------------------------------------------
## 17 FEB 2025
1. training on warehouse map 25 agents
- test results:
- 35 agents
- model throughput: 4.385
- random throughput: 4.32

2. use PBS for hard instances
- fixing implementation
- assert new_start is not None
- after reaching one goal, push to heap (open_set, (heuristic(current, goal), g, current, path)) not (open_set, (heuristic(current, goal), 0, current, path))
- order of actions in st_astar has to match PBS implementation since tie break is going to affect PP
- training on random_w_PBS with 19 agents

3. use Qjt
. see if Qjt has expressive enough by just training Qjt first

4. TODO
- localize reward to 1 step away from pair
- use curriculum learning (number of agents)

## 18 FEB 2025
1. use PBS for hard instances
- fixing implementation
- should only replan for aj where there exist ak < aj in partial prio N

## 19 FEB 2025
1. use PBS for hard instances
- fixed, plan conflict free only for window_size time step

## 22 FEB 2025
1. train again, with corrected PBS and DHC heur layer, use curriculum learning
. on warehouse_1 map (4 rows, 2 col), train-warehouse_1

## 23 FEB 2025
1. generating result for PBS warehouse_2 in gen_results_warehouse2

2. just realised that fulfillment warehouse map should go back and forth, but checked RHCR code and thats not the case. they just keep generating random goal locations from the shelves, not the start cells
- so why is their throughput so low?
- forgot to divide by window_size = 3

## 3 MAR 2025
1. generating results for random maps

2. train on random map using curriculum learning

## 4 MAR 2025
1. generating result for random_PP 70 agents on warehouse_2
. changed np.random.shuffle to random.shuffle

2. 5000 steps means 5000 agent steps, not 5000 * window steps

# meet updates
1. use Qjt
2. use exp(1.1, delay)*10 as reward
- fix range to [0, 10]
- makes Qjt easier to learn and converge to <1
3. Q'jt is very small and V(s) close to reward because there isnt much difference between Q'jt(s, ui) and Q'jt(s, uj), meaning taking joint action ui vs uj dont make that much of a difference

## 6 MAR 2025
1. why is leakyrelu set to inplace=True in encoder? changed to default


## 28 MAR 2025
1. training loss = Ltd to see if Qjt network is sufficient to learn reward
- based on previous experiment, it does work. but after adding Lopt Lnopt it doesnt? wait for more steps and conclude

## 10 APR 2025
Model input update: Added the agent's coordinates as part of the input.
batch size: 32
Exploration strategy: Switched to epsilon-greedy.
Curriculum learning:
    Started training with 50 agents.
    Incrementally added 2 agents each time the loss converged below 3.
    Training stopped at 128 agents (reason unconfirmed, possibly due to increased difficulty).
Training performance:
    Loss converged well (near 0), as shown in the plot.
QTRAN modification:
    Decreased the lambda parameter (weight of Lopt and Lnopt loss).
    Possibly improved effectiveness of Ltd and Qjoint learning (needs further verification).
Input size adjustment: Changed to 11x11 to allow a simulation window of 5 time steps.
Evaluation:
    Compared trained model to random PP; results shown in the second plot.
    Both methods failed on 100-agent testing scenarios.
    RHCR paper achieved up to 140 agents due to longer planning horizon (20 steps vs. your 5 for planning and 5 for execution).


## 11 APR 2025
1. training Qjt only
- reward is sum of delays, see if Qjt is able to converge
- if so, sum of delays is a bigger value, so difference between rewards given a different action should be larger and this will propagate to the individual q values
- also sum of delays is not affected by number of agents, so hopefully delay distribution is more accurate
. if Qjt converges, then add Lopt and Lnopt and learn to decompose the reward, with and without PBS warm start

2. is the building of DirectedGraph not sorted based on confidence this whole time?
- for u, v in list(pair_qval.keys()):
- pair_qval already sorted first

## 13 APR 2025
1. training Qjt only
- didnt converge
. look into it

## 15 APR 2025
1. training Qjt only
- look at input, try adding global state?
- training using huber loss

a. instead of ave reward * 10, times 100 instead? since in a group theres lots of agents
b. train from very small number of agents first

2. PBS training
- using pbs labels fully not good idea?
- randomize the toposort
- change reward to times 100 instead of 10
- too big
- num_agents/len(group)
- back to times 10
- is model able to learn with such a variety of feasible priority order?
- increase batch size? since loss is so noisy

3. train whole using huber loss?
- training

4. is prioritized replay correctly implemented?

5. what happened to pbs at agents = 90

## 23 APR 2025
1. work on penalize when ordering fails
- given -10 penalty to group that fails

2. is buffer implemented correctly? try save to txt every 100 steps the last 500 data points
- looks like it is

3. PBS trained
. train again with penalized order
- start epsilon from 0.8

4. huber loss doesnt seem better

5. what happened to pbs at agents = 90
- using planning window of 20 like in the paper the throughput keeps increasing
- this suggests that using planning window of 5 the paths planned are not optimal because they are shortsighted

6. train non PBS now with penalized failed order

## 24 APR 2025
1. debug, start from small scenarios, confirm that model is working

2. how is Ltd calculated for QTRAN-alt? theres a Qjt term for each agent?

## 29 APR 2025
1. confirmed that model is able to converge on overfit test
. moving to see if Ltd converge on warehouse_0, small warehouse 4 agents

2. LR 1e-3 seems to be doing good for Ltd
. trying hidden dim = 128 since new data cant be predicted well whereas the data in buffer is predicted accurately, tells me that model isnt able to generalize to new data
- increasing hidden dim doesnt seem to help, it gets some delayed predictions correct, i think the problem is that the input is not good enough.

# 3 MAY 2025
1. since i am dividing the loss by the number of groups in an episode, eps with more groups will contribute less loss each group. so if one group in the episode is not doing well, the error is spread among the other groups in that episode. solution, each group has to have equal weight, treated independently.

# 5 MAY 2025
1. is the problem overfitting? or input not good enough?
- try using a simpler model. should help with overfitting

# 6 MAY 2025
1. running small training to see if dev loss per bucket decreases
- fixing the ltd calculation, each group now contributes equally to the loss instead of per episode, so groups in episodes with more groups doesnt contribute less to the loss

2. input global view to Qjt

# 8 MAY 2025
1. train PBS for a very long time, like the first version, until Ltd converge

2. training on small map with 10*ave_reward to see if it converges. uses updated group wise loss
. training on tmux train_small_debug into warehouse_0_a

3. train non PBS for a very long time on warehouse_2, with updated loss calculation on the group level. uses new simpler model. use curriculum learning that starts at 50 agents, and epsilon greedy
. training on tmux train into warehouse_2_update1
. training on tmux ___ into warehouse_2_update1gpu

# 9 MAY 2025
so right now im waiting on the training of the non PBS version with epsilon greedy, trying to see if i get the same better performance in throughput as last time. so i just hv to wait for the results. but from the small map experiment it looks like the Qjt network is not expressive enough? so i think i'll have to use a more complex model. and also in the meantime i can work on including the whole map as the input to the Qjt network so that its able to predict the reward more accurately.

1. train non PBS for a very long time on warehouse_2, with updated loss calculation on the group level. uses new simpler model. use curriculum learning that starts at 50 agents, and epsilon greedy
. training on tmux train into warehouse_2_update1
. training on tmux train_gpu into warehouse_2_update1gpu

2. training on small map with 10*ave_reward to see if it converges. uses updated group wise loss.
- Ltd only first to see if model expressive enough
- epsilon greedy off
- curriculum off
- gpu on
. training on tmux train_small_debug into warehouse_0_a

# 10 MAY 2025
the problem right now is the performance of the model is not much better than just random PP. we can learn the PBS labels so that the model performs close to PBS.
the issue right now is that the model cant even learn the Qjt value accurately. i think this happens after adding the penalty.

1. check if Qjt is better without penalty, just discard infeasible ordering
. training on tmux train_nopenalty into warehouse_2_nopenalty

2. checking if implementation is correct

# 12 MAY 2025
1. tmux train_small_debug into warehouse_0_a
- 175000 steps, Ltd converge, prediction in checkpoint still gets some values wrong.

2. tmux train_gpu into warehouse_2_update1gpu
- 16401 steps, checkpoint doesnt accurately predict Qjt, possibly because model too small, and using -10 confuses the model

3. tmux train into warehouse_2_update1
- 54401 steps, checkpoint doesnt accurately predict Qjt, possibly because model too small, and using -10 confuses the model

4. tmux train_nopenalty into warehouse_2_nopenalty
- 20701 steps, checkpoint not accurate, model possibly too small since penalty is already removed

the Qjt doesnt converge like before. could be because i
1. changed the model or
2. the group wise loss calculation
GOAL: get the model to converge again
TODO now:
- work on the model, get it close to before, at the same time make it efficient (remove python for loops, etc.)
- 